# pipeline/canvas_builder.py
"""
Deterministic Canvas payload builder for Floatchat.
Produces JSON payloads the frontend can render:
 - type: "table"
 - type: "chart" with chartType in {"line","bar","area"}
 - type: "map" (point list)
This module intentionally avoids any LLM content generation.

Design decisions:
- Inline JSON previews are strictly bounded to avoid huge payloads.
- For large datasets a sample CSV (generated by the RAG pipeline) is referenced instead of embedding all rows.
- Column lookups are case-insensitive.
- Outputs include schema_version and minimal provenance.
"""
from pathlib import Path
from typing import Dict, Any, Optional, List, Iterable
import pandas as pd
import numpy as np
import re
import math
import json
import datetime

# ---------------- Configuration - tune as needed ----------------
MAX_TABLE_ROWS = 200            # max rows to embed directly in JSON tables
MAX_CHART_POINTS = 200          # max points to embed directly in chart.data (frontend can load CSV for full data)
SAMPLE_CSV_SUFFIX = ".sample.csv"  # suffix used by pipeline for sample CSV pointer
SCHEMA_VERSION = "1.0"

# Heuristics (simple, explicit)
_TIME_KEYWORDS = {"time", "trend", "over time", "change", "temporal", "date", "juld", "time series", "timeseries"}
_PROFILE_KEYWORDS = {"profile", "depth", "vertical", "pres", "pressure"}
_AGG_KEYWORDS = {"average", "mean", "min", "max", "median", "count", "how many", "number of", "total"}
_MAP_KEYWORDS = {"near", "nearest", "map", "location", "lat", "lon", "latitude", "longitude", "equator", "nearby"}

# ---------------- Helpers ----------------
def _query_tokens(q: str) -> List[str]:
    return re.findall(r"[A-Za-z0-9_]+", (q or "").lower())

def _contains_any(q: str, words: Iterable[str]) -> bool:
    if not q:
        return False
    qlow = q.lower()
    return any(w in qlow for w in words)

def _safe_head(df: pd.DataFrame, n: int = MAX_TABLE_ROWS) -> pd.DataFrame:
    if df is None:
        return pd.DataFrame()
    if df.empty:
        return df.head(0)
    return df.head(n)

def _to_jsonable(v):
    """Convert a scalar or timestamp to a JSON-serializable primitive (or None)."""
    if v is None:
        return None
    # pandas timestamp or numpy datetime
    if isinstance(v, (pd.Timestamp, datetime.datetime, np.datetime64)):
        try:
            return pd.to_datetime(v).isoformat()
        except Exception:
            return str(v)
    # numpy numeric scalars
    if isinstance(v, (np.integer, np.floating)):
        return v.item()
    # python scalar numeric/bool
    if isinstance(v, (int, float, bool)):
        # convert nan -> None
        if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):
            return None
        return v
    # nan-like
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    # fallback: string
    return str(v)

def find_col(df: pd.DataFrame, candidate: str) -> Optional[str]:
    """
    Case-insensitive column finder. Returns the actual column name if matched, else None.
    Exact-match precedence, then contains match.
    """
    if df is None:
        return None
    cand = candidate.lower()
    # exact
    for c in df.columns:
        if c.lower() == cand:
            return c
    # contains
    for c in df.columns:
        if cand in c.lower():
            return c
    return None

# ---------------- Payload Builders ----------------
def build_table_payload(title: str, df: pd.DataFrame, sample_csv_path: Optional[str]=None, max_rows:int=MAX_TABLE_ROWS) -> Dict[str,Any]:
    """Return a 'table' JSON payload (headers + rows). Limited to max_rows inline."""
    if df is None:
        df = pd.DataFrame()
    df2 = _safe_head(df, max_rows)
    headers = [str(c) for c in df2.columns]
    rows = []
    for _, r in df2.iterrows():
        row = []
        for c in headers:
            v = r.get(c, None)
            row.append(_to_jsonable(v))
        rows.append(row)

    payload = {
        "schema_version": SCHEMA_VERSION,
        "type": "table",
        "title": title,
        "headers": headers,
        "rows": rows,
        "row_count": int(len(df)),
        "sample_csv": sample_csv_path or None,
        "provenance": {},  # optional: fill in table name / file identifiers if you have them
    }
    return payload

def _downsample_df_for_chart(df: pd.DataFrame, max_points: int = MAX_CHART_POINTS) -> pd.DataFrame:
    """Downsample by uniform sampling across index to limit points (keeps head/tail distribution)."""
    if df is None or df.empty:
        return df
    n = len(df)
    if n <= max_points:
        return df
    # pick uniform indices and ensure unique sorted indices
    idx = np.linspace(0, n - 1, max_points, dtype=int)
    return df.iloc[idx]

def build_chart_payload(title: str, df: pd.DataFrame, x: str, y: str,
                        chart_type: str = "line", color: Optional[str] = None,
                        sample_csv_path: Optional[str] = None) -> Dict[str,Any]:
    """
    Build a chart payload. df should include x and y columns.
    chart_type: 'line'|'bar'|'area'
    Result includes:
      - data: limited preview (<= MAX_CHART_POINTS)
      - aggregates: min/max/count for quick summary
      - row_count and sample_csv reference for full data
    """
    if df is None or df.empty:
        return {"schema_version": SCHEMA_VERSION, "type":"visualization_unavailable", "reason": "No numeric data available."}

    if x not in df.columns or y not in df.columns:
        return {"schema_version": SCHEMA_VERSION, "type":"visualization_unavailable", "reason": f"Missing columns for chart: need {x} and {y}."}

    # produce downsampled preview
    cols = [x, y] + ([color] if color and color in df.columns else [])
    df2 = df[cols].copy()
    df_preview = _downsample_df_for_chart(df2, max_points=MAX_CHART_POINTS)

    data = []
    for _, r in df_preview.iterrows():
        rec = {}
        rec[x] = _to_jsonable(r[x])
        rec[y] = _to_jsonable(r[y])
        if color and color in df_preview.columns:
            rec[color] = _to_jsonable(r[color])
        data.append(rec)

    # aggregates for quick plotting decisions
    aggregates = {}
    try:
        aggregates["x_min"] = _to_jsonable(df[x].min())
        aggregates["x_max"] = _to_jsonable(df[x].max())
        aggregates["y_min"] = _to_jsonable(df[y].min())
        aggregates["y_max"] = _to_jsonable(df[y].max())
        aggregates["count"] = int(len(df))
    except Exception:
        aggregates = {"count": int(len(df))}

    payload = {
        "schema_version": SCHEMA_VERSION,
        "type":"chart",
        "title": title,
        "chartType": chart_type,
        "xKey": x,
        "yKey": y,
        "data": data,  # limited preview
        "row_count": int(len(df)),
        "sample_csv": sample_csv_path or None,
        "aggregates": aggregates,
        "downsampled": len(df) > len(data),
        "provenance": {}
    }
    if color and color in df.columns:
        payload["color"] = color
    return payload

def build_map_payload(title: str, df: pd.DataFrame, lat_col="LATITUDE", lon_col="LONGITUDE", label_col:Optional[str]=None, sample_csv_path:Optional[str]=None) -> Dict[str,Any]:
    """
    Return a map points payload (list of points).
    Only include points where both lat & lon are present and numeric.
    Also return omitted_count and omitted_reasons for auditing.
    """
    if df is None or df.empty:
        return {"schema_version": SCHEMA_VERSION, "type":"visualization_unavailable", "reason": "Missing time_location/metadata data."}

    # find real column names (case-insensitive)
    lat_c = find_col(df, lat_col)
    lon_c = find_col(df, lon_col)
    lab_c = find_col(df, label_col) if label_col else None

    if lat_c is None or lon_c is None:
        return {"schema_version": SCHEMA_VERSION, "type":"visualization_unavailable", "reason": "Missing lat/lon columns."}

    df2 = _safe_head(df[[lat_c, lon_c] + ([lab_c] if lab_c else [])], MAX_TABLE_ROWS)

    points = []
    omitted_reasons = []
    omitted_count = 0
    for _, r in df2.iterrows():
        lat = r[lat_c]
        lon = r[lon_c]
        if pd.isna(lat) or pd.isna(lon):
            omitted_count += 1
            omitted_reasons.append({"row_index": int(r.name), "reason": "missing lat or lon"})
            continue
        try:
            latf = float(lat)
            lonf = float(lon)
        except Exception:
            omitted_count += 1
            omitted_reasons.append({"row_index": int(r.name), "reason": "non-numeric lat/lon"})
            continue
        pt = {"lat": latf, "lon": lonf}
        if lab_c:
            lab = r[lab_c]
            pt["label"] = None if pd.isna(lab) else str(lab)
        points.append(pt)

    payload = {
        "schema_version": SCHEMA_VERSION,
        "type":"map",
        "title": title,
        "points": points,
        "row_count": int(len(df)),
        "returned_count": len(points),
        "omitted_count": omitted_count,
        "omitted_reasons": omitted_reasons,
        "sample_csv": sample_csv_path or None,
        "provenance": {}
    }
    return payload

# ---------------- Intent resolver ----------------
def resolve_intent(query: str) -> str:
    """
    Return one of: 'map','time_series','profile','aggregate','table'
    Simple keyword heuristics; use unified code path for both canvas and LLM.
    """
    q = (query or "").lower()
    if _contains_any(q, _MAP_KEYWORDS):
        return "map"
    if _contains_any(q, _PROFILE_KEYWORDS):
        return "profile"
    if _contains_any(q, _TIME_KEYWORDS):
        return "time_series"
    if _contains_any(q, _AGG_KEYWORDS):
        return "aggregate"
    # default: table if mentions metadata-type words else numeric/time series
    if any(w in q for w in ["project", "pi", "principal", "platform", "list", "show me", "details"]):
        return "table"
    return "time_series"

# ---------------- Main decision function ----------------
def decide_and_build(query: str,
                     df_numeric: pd.DataFrame,
                     df_metadata: pd.DataFrame,
                     sample_csv: Optional[str] = None) -> Dict[str,Any]:
    """
    Decide the best canvas payload(s) for the query and available data.
    Returns a dict with keys:
      - visuals: list[ payloads ]
      - rationale: string explanation (useful for frontend debug)
    This function is entirely deterministic and should not use LLM.
    """
    visuals: List[Dict[str,Any]] = []
    rationale_items: List[str] = []
    q = (query or "").strip()

    # unify intent
    intent = resolve_intent(q)
    rationale_items.append(f"resolved_intent={intent}")

    # Helper: canonical column lookup functions
    def has_col(df, names: Iterable[str]) -> Optional[str]:
        """Return first matching column name for any candidate in names (case-insensitive)."""
        if df is None:
            return None
        for n in names:
            c = find_col(df, n)
            if c:
                return c
        return None

    # --------------- MAP ----------------
    if intent == "map":
        if df_metadata is not None and not df_metadata.empty:
            rationale_items.append("Detected geographic intent -> build map from time_location/metadata.")
            mp = build_map_payload("Profiles: geographic sample", df_metadata, lat_col="LATITUDE", lon_col="LONGITUDE", label_col="PLATFORM_NUMBER", sample_csv_path=sample_csv)
            visuals.append(mp)
            return {"visuals": visuals, "rationale": "; ".join(rationale_items)}

    # --------------- TIME SERIES ----------------
    if intent == "time_series":
        if df_numeric is not None and not df_numeric.empty:
            # find juld column
            juld_col = has_col(df_numeric, ["juld", "date", "time"])
            preferred_y = has_col(df_numeric, ["TEMP", "PSAL", "PRES", "TEMP_ADJUSTED", "PSAL_ADJUSTED"])
            if juld_col and preferred_y:
                rationale_items.append(f"Detected time-series intent; using y={preferred_y} vs x={juld_col}.")
                # ensure juld col exists and is sorted
                try:
                    df_sorted = df_numeric.sort_values(by=juld_col)
                except Exception:
                    df_sorted = df_numeric
                visuals.append(build_chart_payload(f"{preferred_y} over time", df_sorted, x=juld_col, y=preferred_y, chart_type="line", sample_csv_path=sample_csv))
                return {"visuals": visuals, "rationale": "; ".join(rationale_items)}

    # --------------- PROFILE (depth vs variable) ----------------
    if intent == "profile":
        if df_numeric is not None and not df_numeric.empty:
            pres_col = has_col(df_numeric, ["PRES", "pressure", "depth"])
            temp_col = has_col(df_numeric, ["TEMP", "TEMP_ADJUSTED", "temperature"])
            psal_col = has_col(df_numeric, ["PSAL", "PSAL_ADJUSTED", "salinity"])
            if pres_col and temp_col:
                rationale_items.append(f"Detected profile intent -> PRES vs TEMP ({pres_col} vs {temp_col}).")
                visuals.append(build_chart_payload("Temperature profile (sample)", df_numeric.sort_values(by=pres_col), x=pres_col, y=temp_col, chart_type="line", sample_csv_path=sample_csv))
                return {"visuals": visuals, "rationale": "; ".join(rationale_items)}
            if pres_col and psal_col:
                rationale_items.append(f"Detected profile intent -> PRES vs PSAL ({pres_col} vs {psal_col}).")
                visuals.append(build_chart_payload("Salinity profile (sample)", df_numeric.sort_values(by=pres_col), x=pres_col, y=psal_col, chart_type="line", sample_csv_path=sample_csv))
                return {"visuals": visuals, "rationale": "; ".join(rationale_items)}

    # --------------- AGGREGATION ----------------
    if intent == "aggregate":
        if df_numeric is not None and not df_numeric.empty:
            # pick first meaningful numeric column (prefer PSAL/TEMP/PRES)
            for candidate in ["PSAL_ADJUSTED", "PSAL", "TEMP_ADJUSTED", "TEMP", "PRES"]:
                c = find_col(df_numeric, candidate)
                if c:
                    numeric_col = c
                    break
            else:
                # fallback: any numeric column
                numeric_cols = [col for col in df_numeric.columns if pd.api.types.is_numeric_dtype(df_numeric[col])]
                numeric_col = numeric_cols[0] if numeric_cols else None

            if numeric_col:
                agg_series = df_numeric[numeric_col].agg(["count", "mean", "min", "max"])
                agg_df = pd.DataFrame([agg_series.to_dict()])
                visuals.append(build_table_payload(f"Aggregates for {numeric_col}", agg_df, sample_csv_path=sample_csv))
                rationale_items.append(f"Detected aggregation intent -> aggregates for {numeric_col}")
                return {"visuals": visuals, "rationale": "; ".join(rationale_items)}

    # --------------- FALLBACKS ----------------
    if df_metadata is not None and not df_metadata.empty:
        rationale_items.append("Fallback -> show metadata sample table.")
        visuals.append(build_table_payload("Metadata sample", df_metadata, sample_csv_path=sample_csv))
        return {"visuals": visuals, "rationale": "; ".join(rationale_items)}

    if df_numeric is not None and not df_numeric.empty:
        rationale_items.append("Fallback -> show numeric sample table.")
        visuals.append(build_table_payload("Numeric sample", _safe_head(df_numeric, MAX_TABLE_ROWS), sample_csv_path=sample_csv))
        return {"visuals": visuals, "rationale": "; ".join(rationale_items)}

    # nothing available
    return {"visuals": [], "rationale": "No suitable data for visualization."}
